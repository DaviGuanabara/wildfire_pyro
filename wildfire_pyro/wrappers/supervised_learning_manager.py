import torch
import numpy as np
from typing import Any, Dict, Tuple


from wildfire_pyro.environments.base_environment import BaseEnvironment
from wildfire_pyro.wrappers.base_learning_manager import BaseLearningManager


class SupervisedLearningManager(BaseLearningManager):
    def __init__(
        self,
        neural_network: Any,
        environment: BaseEnvironment,
        parameters: Dict[str, Any],
        batch_size: int = 64,
    ):
        super().__init__(environment, neural_network, parameters)  # Corrigido

        self.batch_size = batch_size
        self.rollout_size = batch_size

        # Inicializa otimizador e loss function corretamente
        self.optimizer = torch.optim.Adam(
            self.neural_network.parameters(), lr=parameters.get("lr", 1e-3)
        )
        self.loss_func = torch.nn.MSELoss()


    


        
    def learn(self, total_steps: int):
        """
        Main learning loop.
        Alternates between collecting rollouts and training the neural network.
        """
        super().learn(total_steps)
        
    def predict(
        self, obs: np.ndarray, deterministic: bool = True
    ) -> Tuple[np.ndarray, Any]:
        """
        Makes predictions using the trained model.

        Args:
            obs (np.ndarray): Current observation. Can be a single observation or a batch.
            deterministic (bool, optional): If True, use deterministic prediction. Defaults to True.

        Returns:
            Tuple[np.ndarray, Any]: Predicted action(s) and additional information (empty dict).
        """

        # Set the network to evaluation mode
        self.neural_network.eval()
        with torch.no_grad():
            # Convert observation to tensor and move to the correct device

            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device)
            expected_obs_shape = self.environment.observation_space.shape
            is_batch = len(obs_tensor.shape) == len(expected_obs_shape) + 1

            # Add batch dimension if not in batch format
            if not is_batch:
                # (1, num_neighbors, feature_dim)
                obs_tensor = obs_tensor.unsqueeze(0)

            # (batch_size, output_dim)
            action_tensor = self.neural_network(obs_tensor)
            action = action_tensor.cpu().numpy()

            # If input was not in batch, remove the batch dimension from the
            # output
            if not is_batch:
                # (output_dim,)
                action = action.squeeze(0)

        # Return action(s) and an empty dictionary for additional information

        return action, {}
    
    def _train(self) -> float:
        """
        Trains the neural network using data from the buffer.

        Returns:
            float: Average training loss.
        """

        self.neural_network.train()
        buffer_size = self.buffer.size()
        if buffer_size < self.batch_size:
            print("[Warning] Not enough data in buffer to train. Skipping training.")
            return 0.0

        # Sample a batch (entire buffer)

        observations, _, ground_truths = self.buffer.sample_batch(
            self.batch_size)

        # (batch_size, num_neighbors, feature_dim)
        observations = observations.to(self.device)
        # (batch_size, 1)
        ground_truths = ground_truths.to(self.device)

        self.optimizer.zero_grad()

        # necessary to garanty that the actions are related to the current model.
        # (batch_size, output_dim)
        # `y_pred` is the predicted output generated by the neural network model
        # (`self.neural_network`) when given the input observations (`observations`). It represents
        # the model's prediction for the ground truth values (`ground_truths`) based on the input
        # data. The model is trained to minimize the difference between `y_pred` and the actual ground
        # truth values during the training process.
        y_pred = self.neural_network(observations)

        loss = self.loss_func(y_pred, ground_truths)
        loss.backward()
        self.optimizer.step()

        average_loss: float = loss.item()
        # print(f"[INFO] Train Loss: {average_loss:.4f}")

        return average_loss
