import torch
import numpy as np
from typing import Any, Dict, Optional, Tuple


from wildfire_pyro.environments.base_environment import BaseEnvironment
from wildfire_pyro.wrappers.base_learning_manager import BaseLearningManager
from wildfire_pyro.wrappers.components.target_provider import InfoFieldTargetProvider, BaseTargetProvider
from .components import predict_model

class SupervisedLearningManager(BaseLearningManager):
    def __init__(
        self,
        neural_network: torch.nn.Module,
        environment: BaseEnvironment,
        logging_parameters=Dict[str, Any],
        runtime_parameters=Dict[str, Any],
        model_parameters=Dict[str, Any],
    ):
        
        
        super().__init__(environment, neural_network, logging_parameters=logging_parameters, runtime_parameters=runtime_parameters,
                         model_parameters=model_parameters)


        # Inicializa otimizador e loss function corretamente
        self.optimizer = torch.optim.Adam(
            self.neural_network.parameters(), lr=self.lr
        )
        self.loss_func = torch.nn.MSELoss()
        
    def predict(
        self, obs: np.ndarray, deterministic: bool = True
    ) -> Tuple[np.ndarray, Any]:
        """
        Makes predictions using the trained model.

        Args:
            obs (np.ndarray): Current observation. Can be a single observation or a batch.
            deterministic (bool, optional): If True, use deterministic prediction. Defaults to True.

        Returns:
            Tuple[np.ndarray, Any]: Predicted action(s) and additional information (empty dict).
        """

        return predict_model(
            self.neural_network,
            obs,
            self.device,
            input_shape=self.environment.observation_space.shape,
        )

    
    def _train(self) -> float:
        """
        Trains the neural network using data from the buffer.

        Returns:
            float: Average training loss.
        """

        self.neural_network.train()
        buffer_size = self.buffer.size()
        if buffer_size < self.batch_size:
            print("[Warning] Not enough data in buffer to train. Skipping training.")
            return 0.0

        # Sample a batch (entire buffer)

        observations, _, targets = self.buffer.sample_batch(
            self.batch_size)

        # (batch_size, num_neighbors, feature_dim)
        observations = observations.to(self.device)
        # (batch_size, 1)
        targets = targets.to(self.device)

        self.optimizer.zero_grad()

        # necessary to garanty that the actions are related to the current model.
        # (batch_size, output_dim)
        # `y_pred` is the predicted output generated by the neural network model
        # (`self.neural_network`) when given the input observations (`observations`). It represents
        # the model's prediction for the ground truth values (`targets`) based on the input
        # data. The model is trained to minimize the difference between `y_pred` and the actual ground
        # truth values during the training process.
        y_preds = self.neural_network(observations)

        loss = self.loss_func(y_preds, targets)
        loss.backward()
        self.optimizer.step()

        average_loss: float = loss.item()
        # print(f"[INFO] Train Loss: {average_loss:.4f}")

        return average_loss